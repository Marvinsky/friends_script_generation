{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if train_on_gpu:\n",
    "    print(\"Training on GPU\")\n",
    "else:\n",
    "    print(\"Training on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Scene: Barbados, Monica and Chandler's Room. They both enter from Ross's room. Monica still has her\n",
      "[Scene: Barbados, Monica and Chandler's Room. They both enter from Ross's room. Monica still has her\n",
      "416013\n"
     ]
    }
   ],
   "source": [
    "with open(\"friends10.txt\", \"r\") as f:\n",
    "    dialogs = f.read()\n",
    "text = dialogs[:]\n",
    "print(dialogs[:100])\n",
    "print(text[:100])\n",
    "print(len(dialogs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roughly the number of unique words: 10760\n",
      "Number of lines: 12389\n",
      "Average number of word in each line: 5.975704253773508\n",
      "\n",
      "The lines from 0 to 10\n",
      "[Scene: Barbados, Monica and Chandler's Room. They both enter from Ross's room. Monica still has her big, frizzy hair.]\n",
      "\n",
      "Monica: Oh, the way you crushed Mike at ping pong was such a turn-on.You wanna...? (plays with her finger on Chandlers chest)\n",
      "\n",
      "Chandler: You know, I'd love to, but I'm a little tired.\n",
      "\n",
      "Monica: I'll put a pillowcase over my head.\n",
      "\n",
      "Chandler: You're on!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "view_line_range = (0, 10)\n",
    "#roughly the number of unique words\n",
    "print(\"Roughly the number of unique words: {}\".format(len({word:None for word in dialogs.split()})))\n",
    "lines = dialogs.split(\"\\n\")\n",
    "print(\"Number of lines: {}\".format(len(lines)))\n",
    "word_count_line = [len(line.split()) for line in lines]\n",
    "print(\"Average number of word in each line: {}\".format(np.average(word_count_line)))\n",
    "print()\n",
    "print(\"The lines from {} to {}\".format(*view_line_range))\n",
    "print(\"\\n\".join(dialogs.split(\"\\n\")[view_line_range[0]:view_line_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    counts = Counter(text)\n",
    "    vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "    vocab_to_int = {word:ii for ii, word in enumerate(vocab, 1)}\n",
    "    int_to_vocab = {ii: word for word, ii in vocab_to_int.items()}\n",
    "    return (vocab_to_int, int_to_vocab)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_lookup():\n",
    "    symbols = [\".\", \",\", \"\\\"\", \";\", \"!\", \"?\", \"(\", \")\", \"-\", \"\\n\"]\n",
    "    punctuations = [\"||Period||\",\n",
    "                    \"||Comma||\",\n",
    "                    \"||Quotation_Mark||\",\n",
    "                    \"||Semicolon||\",\n",
    "                    \"||Exclamation_Mark||\",\n",
    "                    \"||Question_Mark||\",\n",
    "                    \"||Left_Parentheses||\",\n",
    "                    \"||Right_Parentheses||\",\n",
    "                    \"||Dash||\",\n",
    "                    \"||Return||\"\n",
    "                   ]\n",
    "    d = {symbol:token for symbol, token in zip(symbols, punctuations)}\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "SPECIAL_WORDS = {'PADDING': '<PAD>'}\n",
    "token_dict = token_lookup()\n",
    "\n",
    "if os.path.exists('preprocessfriends.p'):\n",
    "    int_text, vocab_to_int, int_to_vocab, token_dict = pickle.load(open('preprocessfriends.p', mode='rb'))\n",
    "else:\n",
    "    for key, token in token_dict.items():\n",
    "        text = text.replace(key, ' {} '.format(token))\n",
    "\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "\n",
    "    vocab_to_int, int_to_vocab = create_lookup_tables(text + list(SPECIAL_WORDS.values()))\n",
    "    int_text = [vocab_to_int[word] for word in text]\n",
    "    pickle.dump((int_text, vocab_to_int, int_to_vocab, token_dict), open('preprocessfriends.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def batch_data(words, sequence_length, batch_size):\n",
    "    \"\"\"\n",
    "    Batch the neural network data using DataLoader\n",
    "    :param words: The word ids of the Friends TV scripts\n",
    "    :param sequence_length: The sequence length of each batch\n",
    "    :param batch_size: The size of each batch; the number of sequences in a batch\n",
    "    :return: DataLoader with batched data\n",
    "    \"\"\"\n",
    "    # TODO: Implement function\n",
    "    total_batches = batch_size*sequence_length\n",
    "    n_batches = len(words)//total_batches\n",
    "    words = words[:n_batches*total_batches]\n",
    "\n",
    "    features, target = [], []\n",
    "    for ii in range(0, len(words)):\n",
    "        if ii+sequence_length < len(words):\n",
    "            features.append(words[ii:ii+sequence_length])\n",
    "            target.append(words[ii+sequence_length])\n",
    "    \n",
    "    data = TensorDataset(torch.tensor(features), torch.tensor(target))\n",
    "    data_loader = DataLoader(data, shuffle = True, batch_size=batch_size)\n",
    "    \n",
    "    # return a dataloader\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5])\n",
      "tensor([[30, 31, 32, 33, 34],\n",
      "        [20, 21, 22, 23, 24],\n",
      "        [11, 12, 13, 14, 15],\n",
      "        [21, 22, 23, 24, 25],\n",
      "        [24, 25, 26, 27, 28],\n",
      "        [15, 16, 17, 18, 19],\n",
      "        [23, 24, 25, 26, 27],\n",
      "        [38, 39, 40, 41, 42],\n",
      "        [ 2,  3,  4,  5,  6],\n",
      "        [22, 23, 24, 25, 26]])\n",
      "\n",
      "torch.Size([10])\n",
      "tensor([35, 25, 16, 26, 29, 20, 28, 43,  7, 27])\n"
     ]
    }
   ],
   "source": [
    "test_text = range(50)\n",
    "t_loader = batch_data(test_text, sequence_length=5, batch_size=10)\n",
    "\n",
    "data_iter = iter(t_loader)\n",
    "sample_x, sample_y = data_iter.next()\n",
    "\n",
    "print(sample_x.shape)\n",
    "print(sample_x)\n",
    "print()\n",
    "print(sample_y.shape)\n",
    "print(sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the PyTorch RNN Module\n",
    "        :param vocab_size: The number of input dimensions of the neural network (the size of the vocabulary)\n",
    "        :param output_size: The number of output dimensions of the neural network\n",
    "        :param embedding_dim: The size of embeddings, should you choose to use them        \n",
    "        :param hidden_dim: The size of the hidden layer outputs\n",
    "        :param dropout: dropout to add in between LSTM/GRU layers\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "        # TODO: Implement function\n",
    "        # set class variables\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        #embedding and LSTM layers: Given the big amount of words is better to use embedding layer as a lookup table\n",
    "        # define model layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                            hidden_dim, \n",
    "                            n_layers,\n",
    "                            dropout = dropout,\n",
    "                            batch_first=True)\n",
    "        #dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        #linear layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        #self.sig = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    \n",
    "    def forward(self, nn_input, hidden):\n",
    "        \"\"\"\n",
    "        Forward propagation of the neural network\n",
    "        :param nn_input: The input to the neural network\n",
    "        :param hidden: The hidden state        \n",
    "        :return: Two Tensors, the output of the neural network and the latest hidden state\n",
    "        \"\"\"\n",
    "        # TODO: Implement function   \n",
    "        batch_size = nn_input.size(0)\n",
    "        \n",
    "        #embedding and lstm_out\n",
    "        embeds = self.embedding(nn_input)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        #stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        #dropout and fully connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        #sigmoid function\n",
    "        #sig_out = self.sig(out)\n",
    "        \n",
    "        #reshape to the batch_size first\n",
    "        out = out.view(batch_size, -1, self.output_size)\n",
    "        out = out[:,-1] # get the last batch of labels\n",
    "\n",
    "        # return one batch of output word scores and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        '''\n",
    "        Initialize the hidden state of an LSTM/GRU\n",
    "        :param batch_size: The batch_size of the hidden state\n",
    "        :return: hidden state of dims (n_layers, batch_size, hidden_dim)\n",
    "        '''\n",
    "        # Implement function\n",
    "        \n",
    "        # initialize hidden state with zero weights, and move to GPU if available\n",
    "        \n",
    "        weight = next(self.parameters()).data\n",
    "        if train_on_gpu:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "            \n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Forward and backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n",
    "    \"\"\"\n",
    "    Forward and backward propagation on the neural network\n",
    "    :param rnn: The PyTorch Module that holds the neural network\n",
    "    :param optimizer: The PyTorch optimizer for the neural network\n",
    "    :param criterion: The PyTorch loss function\n",
    "    :param inp: A batch of input to the neural network\n",
    "    :param target: The target output for the batch of input\n",
    "    :return: The loss and the latest hidden state Tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    # move data to GPU, if available\n",
    "    if train_on_gpu:\n",
    "        #rnn = rnn.cuda()\n",
    "        inp, target = inp.cuda(), target.cuda()\n",
    "    \n",
    "    # perform backpropagation and optimization\n",
    "    hidden = tuple([each.data for each in hidden])\n",
    "    rnn.zero_grad()\n",
    "    output, hidden = rnn.forward(inp, hidden)\n",
    "    loss = criterion(output.squeeze(), target.long())\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(rnn.parameters(), 5)\n",
    "    optimizer.step()\n",
    "    \n",
    "    # return the loss over a batch and the hidden state produced by our model\n",
    "    return loss.item(), hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n",
    "    batch_losses = []\n",
    "    \n",
    "    rnn.train()\n",
    "\n",
    "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
    "    for epoch_i in range(1, n_epochs + 1):\n",
    "        \n",
    "        # initialize hidden state\n",
    "        hidden = rnn.init_hidden(batch_size)\n",
    "        \n",
    "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            \n",
    "            # make sure you iterate over completely full batches, only\n",
    "            n_batches = len(train_loader.dataset)//batch_size\n",
    "            if(batch_i > n_batches):\n",
    "                break\n",
    "            \n",
    "            # forward, back prop\n",
    "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)          \n",
    "            # record loss\n",
    "            batch_losses.append(loss)\n",
    "\n",
    "            # printing loss stats\n",
    "            if batch_i % show_every_n_batches == 0:\n",
    "                print('Epoch: {:>4}/{:<4}  Loss: {}\\n'.format(\n",
    "                    epoch_i, n_epochs, np.average(batch_losses)))\n",
    "                batch_losses = []\n",
    "\n",
    "    # returns a trained rnn\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization And Model Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Set sequence_length to the length of a sequence.\n",
    "* Set batch_size to the batch size.\n",
    "* Set num_epochs to the number of epochs to train for.\n",
    "* Set learning_rate to the learning rate for an Adam optimizer.\n",
    "* Set vocab_size to the number of unique tokens in our vocabulary.\n",
    "* Set output_size to the desired size of the output.\n",
    "* Set embedding_dim to the embedding dimension; smaller than the vocab_size.\n",
    "* Set hidden_dim to the hidden dimension of your RNN.\n",
    "* Set n_layers to the number of layers/cells in your RNN.\n",
    "* Set show_every_n_batches to the number of batches at which the neural network should print progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config1\n",
    "# Sequence Length\n",
    "sequence_length = 50  # of words in a sequence\n",
    "# Batch Size\n",
    "batch_size = 128\n",
    "\n",
    "# data loader - do not change\n",
    "train_loader = batch_data(int_text, sequence_length, batch_size)\n",
    "# Training parameters\n",
    "# Number of Epochs\n",
    "num_epochs = 5\n",
    "# Learning Rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Model parameters\n",
    "# Vocab size\n",
    "vocab_size = len(vocab_to_int) + 1\n",
    "# Output size\n",
    "output_size = len(set(vocab_to_int))+1\n",
    "# Embedding Dimension\n",
    "embedding_dim = 400\n",
    "# Hidden Dimension\n",
    "hidden_dim = 256\n",
    "# Number of RNN Layers\n",
    "n_layers = 2\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5 epoch(s)...\n",
      "Epoch:    1/5     Loss: 4.938925834655762\n",
      "\n",
      "Epoch:    2/5     Loss: 4.528153651454283\n",
      "\n",
      "Epoch:    3/5     Loss: 4.310648696863469\n",
      "\n",
      "Epoch:    4/5     Loss: 4.175825493917027\n",
      "\n",
      "Epoch:    5/5     Loss: 4.108616657054887\n",
      "\n",
      "Elapsed time: 2.0 min, 144.26672387123108 sec\n",
      "Model Trained and Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marvin/anaconda3/lib/python3.6/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "# create model and move to gpu if available\n",
    "rnn1 = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\n",
    "if train_on_gpu:\n",
    "    rnn1.cuda()\n",
    "\n",
    "# defining loss and optimization functions for training\n",
    "optimizer = torch.optim.Adam(rnn1.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# training the model\n",
    "trained_rnn1 = train_rnn(rnn1, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)\n",
    "\n",
    "end = time.time()\n",
    "elapsed_time = end - start\n",
    "print(\"Elapsed time: {} min, {} sec\".format(elapsed_time//60, elapsed_time))\n",
    "\n",
    "# saving the trained model\n",
    "torch.save(trained_rnn1, 'trained_rnn_config1')\n",
    "print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data params\n",
    "# Sequence Length\n",
    "sequence_length = 10  # of words in a sequence\n",
    "# Batch Size\n",
    "batch_size = 256\n",
    "\n",
    "# data loader - do not change\n",
    "train_loader = batch_data(int_text, sequence_length, batch_size)\n",
    "# Training parameters\n",
    "# Number of Epochs\n",
    "num_epochs = 10\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model parameters\n",
    "# Vocab size\n",
    "vocab_size = len(vocab_to_int) + 1\n",
    "# Output size\n",
    "output_size = len(set(vocab_to_int))+1\n",
    "# Embedding Dimension\n",
    "embedding_dim = 400\n",
    "# Hidden Dimension\n",
    "hidden_dim = 256\n",
    "# Number of RNN Layers\n",
    "n_layers = 2\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10 epoch(s)...\n",
      "Elapsed time: 1.0 min, 63.37096166610718 sec\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "# create model and move to gpu if available\n",
    "rnn2 = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\n",
    "if train_on_gpu:\n",
    "    rnn2.cuda()\n",
    "\n",
    "# defining loss and optimization functions for training\n",
    "optimizer = torch.optim.Adam(rnn2.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# training the model\n",
    "trained_rnn2 = train_rnn(rnn2, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)\n",
    "\n",
    "end = time.time()\n",
    "elapsed_time = end - start\n",
    "print(\"Elapsed time: {} min, {} sec\".format(elapsed_time//60, elapsed_time))\n",
    "\n",
    "# saving the trained model\n",
    "torch.save(trained_rnn2, 'trained_rnn_config2')\n",
    "print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
